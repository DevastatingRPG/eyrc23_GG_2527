{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import imutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from task_2a import detect_ArUco_details, mark_ArUco_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m org2 \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(img)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ArUco_details_dict, ArUco_corners = detect_ArUco_details(img)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# img = mark_ArUco_image(img, ArUco_details_dict, ArUco_corners) \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# cv2.imshow(\"Marked Image\",img)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMarked Image2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = cv2.imread('test2.jpg')\n",
    "\n",
    "img = imutils.resize(img, width=960)\n",
    "\n",
    "org2 = copy.deepcopy(img)\n",
    "\n",
    "# ArUco_details_dict, ArUco_corners = detect_ArUco_details(img)\n",
    "# img = mark_ArUco_image(img, ArUco_details_dict, ArUco_corners) \n",
    "\n",
    "\n",
    "# cv2.imshow(\"Marked Image\",img)\n",
    "cv2.imshow(\"Marked Image2\",img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ArUco_corners' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mArUco_corners\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ArUco_corners' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(ArUco_corners))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom Left Event : 2 C(x-75), 2 C(y+ 75)\n",
    "Top Left Event : 22 C(y+20), 15 C(y-20)\n",
    "Mid Left Event : 5 C(x-100), 5 C(y+60)\n",
    "Top Right Event : 54 C(x+50), 39 C(x-50)\n",
    "Bottom Right Event : 53 C, 0 C(y+60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ArUco_corners[55])\n",
    "marking_img = np.copy(org2)\n",
    "centers = copy.deepcopy(ArUco_details_dict)\n",
    "corners = copy.deepcopy(ArUco_corners)\n",
    "events = [\n",
    "    [[corners[7][1][0], corners[21][0][1]], [corners[21][0][0], corners[7][1][1]-10]],\n",
    "    [corners[28][1], corners[14][0]],\n",
    "    [corners[31][1], corners[11][3]], \n",
    "    [[corners[25][0][0], corners[34][0][1]], [corners[34][0][0], corners[25][0][1]]],    \n",
    "    [corners[54][2], corners[40][0]]   \n",
    "]\n",
    "\n",
    "i=0\n",
    "eventlist=[]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = torch.load(\"model.tf\")\n",
    "model.eval()\n",
    "# image = np.expand_dims(image_resized, axis=0)\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224), antialias=False),\n",
    "        \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "for tl, br in events:\n",
    "    tl_adj = [tl[0] + 10, tl[1] + 7]\n",
    "    br_adj = [br[0] - 10, br[1] - 4]\n",
    "    roi = org2[tl_adj[1]:br_adj[1], tl_adj[0]:br_adj[0]]\n",
    "    i+=1\n",
    "\n",
    "    # Apply a blur to the image\n",
    "    blurred = cv2.blur(roi, (5, 5))\n",
    "    # Apply a bilateral filter to the image\n",
    "    filtered = cv2.bilateralFilter(roi, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    # Perform morphological opening\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opened = cv2.morphologyEx(roi, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    gray = cv2.cvtColor(opened, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply adaptive thresholding to the image\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Find contours in the image\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours by area in descending order, take the first one (the largest)\n",
    "    contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "\n",
    "    # Get the bounding rectangle of the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "    # Crop the image using the bounding rectangle, add some padding if needed\n",
    "    padding = 0  # adjust this value according to your needs\n",
    "    crop = roi[max(0, y-padding):min(y+h+padding, roi.shape[0]), max(0, x-padding):min(x+w+padding, roi.shape[1])]\n",
    "    \n",
    "\n",
    "    offset_x = tl_adj[0] + x\n",
    "    offset_y = tl_adj[1] + y    \n",
    "\n",
    "\n",
    "    eventlist.append(crop)\n",
    "    sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "    path = \"EDSR_x4.pb\"\n",
    "    \n",
    "    sr.readModel(path)\n",
    "    \n",
    "    sr.setModel(\"edsr\",4)\n",
    "    \n",
    "    result = sr.upsample(crop)\n",
    "    with torch.inference_mode():\n",
    "        # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = image_transform(result).unsqueeze(dim=0)\n",
    "        # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # 9. Convert prediction probabilities -> prediction labels\n",
    "    pred = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # pred = model.predict(image)\n",
    "    class_names = ['combat', 'destroyedbuilding', 'fire', 'humanitarianaid', 'militaryvehicles']\n",
    "    event = class_names[pred]\n",
    "\n",
    "\n",
    "    offset_x -= 10\n",
    "    offset_y -= 10\n",
    "    box = cv2.rectangle(marking_img, (offset_x, offset_y), (offset_x + w + 20, offset_y + h + 20), (0, 255, 0), 2)\n",
    "    \n",
    "    offset_y -= 10\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    scale = 1\n",
    "    thickness = 2\n",
    "    text = event\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, font, scale, thickness)\n",
    "\n",
    "    cv2.rectangle(marking_img, (offset_x, offset_y - text_height - 10), (offset_x + text_width, offset_y), (140, 133, 133), -1)\n",
    "    cv2.putText(box, event, (offset_x, offset_y - 10), cv2.FONT_HERSHEY_SIMPLEX, scale, (0,255,0), thickness)\n",
    "\n",
    "cv2.imshow(\"Original Image\", marking_img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GG_2527",
   "language": "python",
   "name": "gg_2527"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
