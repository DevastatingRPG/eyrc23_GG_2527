{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms  \n",
    "import imutils     \n",
    "import ctypes\n",
    "from cv2 import aruco  \n",
    "# import RRDBNet_arch as arch     \n",
    "from torchvision.models import efficientnet_v2_s\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.models import resnet50\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*****************************************************************************************\n",
    "*\n",
    "*        \t\t===============================================\n",
    "*           \t\tGeo Guide (GG) Theme (eYRC 2023-24)\n",
    "*        \t\t===============================================\n",
    "*\n",
    "*  This script is to implement Task 4A of Geo Guide (GG) Theme (eYRC 2023-24).\n",
    "*  \n",
    "*  This software is made available on an \"AS IS WHERE IS BASIS\".\n",
    "*  Licensee/end user indemnifies and will keep e-Yantra indemnified from\n",
    "*  any and all claim(s) that emanate from the use of the Software or \n",
    "*  breach of the terms of this agreement.\n",
    "*\n",
    "*****************************************************************************************\n",
    "'''\n",
    "\n",
    "# Team ID:\t\t\t2527\n",
    "# Author List:\t\tShubham, Aditya\n",
    "# Filename:\t\t\ttask_4a.py\n",
    "\n",
    "\n",
    "####################### IMPORT MODULES #######################\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# import time\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms  \n",
    "# import imutils     \n",
    "# import ctypes\n",
    "# from cv2 import aruco  \n",
    "# import RRDBNet_arch as arch     \n",
    "# from torchvision.models import efficientnet_v2_s\n",
    "\n",
    "##############################################################\n",
    "\n",
    "\n",
    "\n",
    "################# ADD UTILITY FUNCTIONS HERE #################\n",
    "def detect_ArUco_details(image):\n",
    "    ArUco_details_dict = {}\n",
    "    ArUco_corners = {}\n",
    "    \n",
    "    ##############\tADD YOUR CODE HERE\t##############\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "    arucoParams = aruco.DetectorParameters()\n",
    "    # GrayScale Conversion\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ArUco markers\n",
    "    corners, ids, _ = aruco.detectMarkers(gray_image, aruco_dict, parameters=arucoParams)\n",
    "\n",
    "    if ids is not None:\n",
    "        for i in range(len(ids)):\n",
    "            marker_id = int(ids[i][0])\n",
    "            marker_center = [int(coord) for coord in list(np.mean(corners[i][0], axis=0).astype(int))]\n",
    "\n",
    "            # Store details in dictionaries\n",
    "            ArUco_details_dict[marker_id] = marker_center\n",
    "            ArUco_corners[marker_id] = [[int(corner[0]), int(corner[1])] for corner in corners[i][0]]\n",
    "    ##################################################\n",
    "    \n",
    "    return ArUco_details_dict, ArUco_corners \n",
    "\n",
    "##############################################################\n",
    "\n",
    "\n",
    "def task_4a_return():\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    ---\n",
    "    Only for returning the final dictionary variable\n",
    "    \n",
    "    Arguments:\n",
    "    ---\n",
    "    You are not allowed to define any input arguments for this function. You can \n",
    "    return the dictionary from a user-defined function and just call the \n",
    "    function here\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `identified_labels` : { dictionary }\n",
    "        dictionary containing the labels of the events detected\n",
    "    \"\"\"  \n",
    "    identified_labels = {}  \n",
    "    \n",
    "##############\tADD YOUR CODE HERE\t##############\n",
    "    # Get screen size\n",
    "    user32 = ctypes.windll.user32\n",
    "    screen_width = user32.GetSystemMetrics(0)\n",
    "    screen_height = user32.GetSystemMetrics(1)\n",
    "    \n",
    "    # Open the camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "    # Try to set exposure, white balance, and other properties\n",
    "    cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 3)  # 0.25 means \"manual exposure, manual iris\"\n",
    "    # cap.set(cv2.CAP_PROP_EXPOSURE, -10)  # Change this value to adjust exposure\n",
    "    cap.set(cv2.CAP_PROP_AUTO_WB, 1)  # 0 means \"disable auto white balance\"\n",
    "    # cap.set(cv2.CAP_PROP_WB_TEMPERATURE, 1000)  # Change this value to adjust white balance  \n",
    "\n",
    "    # Check if the camera is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Unable to open the camera\")\n",
    "        exit()\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "    # Create a named window\n",
    "    cv2.namedWindow(\"Live Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    new_width = screen_width // 2\n",
    "    new_height = frame.shape[0] * new_width // frame.shape[1]\n",
    "\n",
    "    # Set the window size to half of the screen size\n",
    "    cv2.resizeWindow(\"Live Feed\", new_width, new_height)\n",
    "\n",
    "    # Flag to check if the picture has been taken\n",
    "    picture_taken = False\n",
    "\n",
    "    # Get start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read and display frames from the camera\n",
    "\n",
    "    # while not picture_taken:\n",
    "    #     ret, frame = cap.read()\n",
    "    #     frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "    #     if not ret:\n",
    "    #         print(\"Error reading frame from the camera\")\n",
    "    #         break\n",
    "\n",
    "    #     # Resize the frame to half of the screen width\n",
    "    #     frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    #     cv2.imshow(\"Live Feed\", frame)\n",
    "\n",
    "    #     # Move the window to the left\n",
    "    #     cv2.moveWindow(\"Live Feed\", 0, 0)\n",
    "\n",
    "    #     if time.time() - start_time >= 5:\n",
    "    #         cv2.imwrite('eval.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "    #         picture_taken = True\n",
    "\n",
    "    #     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #         break\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    img = cv2.imread(\"images/evalpic.jpg\")\n",
    "    # img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    img = imutils.resize(img, width=960)\n",
    "    marking_img = np.copy(img)\n",
    "    cv2.imshow(\"Marked Image\", marking_img)\n",
    "    # Move the window to the left\n",
    "    cv2.moveWindow(\"Marked Image\", 0, 0)\n",
    "    cv2.waitKey(500)  # delay for 500 milliseconds\n",
    "    _, corners = detect_ArUco_details(marking_img)\n",
    "    \n",
    "    events = [\n",
    "        [[corners[7][1][0], corners[21][0][1]], [corners[21][0][0], corners[7][1][1]-10]],\n",
    "        [corners[28][1], corners[14][0]],\n",
    "        [corners[31][1], corners[11][3]], \n",
    "        [[corners[25][0][0], corners[34][0][1]], [corners[34][0][0], corners[25][0][1]]],    \n",
    "        [corners[54][2], corners[40][0]]   \n",
    "    ]\n",
    "\n",
    "    i=1\n",
    "    eventlist=[]\n",
    "    letters = {1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\", 5: \"E\"}\n",
    "    classconv = { \"combat\": \"Combat\", \"destroyedbuilding\": \"Destroyed buildings\", \n",
    "                 \"humanitarianaid\": \"Humanitarian Aid and rehabilitation\",\n",
    "                 \"militaryvehicles\": \"Military Vehicles\", \"fire\": \"Fire\"}\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = efficientnet_v2_s().to(device)\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=5, bias=True),\n",
    "    ).to(device)\n",
    "\n",
    "    # model.load_state_dict(torch.load('w4.tf'))\n",
    "    model.load_state_dict(torch.load('weights/weights.tf'))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224), antialias=False),\n",
    "            \n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    # image_transform = transforms.Compose([\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Resize(224),\n",
    "    #     # transforms.CenterCrop(224),\n",
    "    #     # transforms.RandomResizedCrop(224),\n",
    "    #     # transforms.RandomHorizontalFlip(),\n",
    "    #     # transforms.RandomVerticalFlip(),\n",
    "    #     # transforms.RandomRotation(10),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # ])\n",
    "    \n",
    "\n",
    "    # model_path = 'RRDB_ESRGAN_x4.pth'  # models/RRDB_ESRGAN_x4.pth OR models/RRDB_PSNR_x4.pth\n",
    "\n",
    "    # modelup = arch.RRDBNet(3, 3, 64, 23, gc=32)\n",
    "    # modelup.load_state_dict(torch.load(model_path), strict=True)\n",
    "    # modelup.eval()\n",
    "    # modelup = modelup.to(device)\n",
    "\n",
    "    temp = 'output/temp.jpg'\n",
    "\n",
    "    for tl, br in events:\n",
    "        tl_adj = [tl[0] + 10, tl[1] + 7]\n",
    "        br_adj = [br[0] - 10, br[1] - 4]\n",
    "        roi = img[tl_adj[1]:br_adj[1], tl_adj[0]:br_adj[0]]\n",
    "        \n",
    "        # Perform morphological opening\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        opened = cv2.morphologyEx(roi, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        gray = cv2.cvtColor(opened, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply adaptive thresholding to the image\n",
    "        binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "        # Find contours in the image\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Sort contours by area in descending order, take the first one (the largest)\n",
    "        contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "\n",
    "        # Get the bounding rectangle of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Crop the image using the bounding rectangle, add some padding if needed\n",
    "        padding = 0  # adjust this value according to your needs\n",
    "        crop = roi[max(0, y-padding):min(y+h+padding, roi.shape[0]), max(0, x-padding):min(x+w+padding, roi.shape[1])]\n",
    "        \n",
    "\n",
    "        offset_x = tl_adj[0] + x\n",
    "        offset_y = tl_adj[1] + y    \n",
    "\n",
    "\n",
    "        eventlist.append(crop)\n",
    "        cv2.imwrite(temp, crop, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "        result = cv2.imread(temp, cv2.IMREAD_COLOR)\n",
    "        # result = cv2.resize(result, (224, 224))\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "            transformed_image = image_transform(result).unsqueeze(dim=0)\n",
    "            # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
    "            target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "        # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "        target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "        # 9. Convert prediction probabilities -> prediction labels\n",
    "        pred = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "        class_names = ['combat', 'destroyedbuilding', 'fire', 'humanitarianaid', 'militaryvehicles']\n",
    "        event = class_names[pred]\n",
    "\n",
    "        print(target_image_pred)\n",
    "\n",
    "\n",
    "        offset_x -= 10\n",
    "        offset_y -= 10\n",
    "        box = cv2.rectangle(marking_img, (offset_x, offset_y), (offset_x + w + 20, offset_y + h + 20), (0, 255, 0), 2)\n",
    "        \n",
    "        offset_y -= 10\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        scale = 1\n",
    "        thickness = 2\n",
    "        text = event\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, font, scale, thickness)\n",
    "\n",
    "        cv2.rectangle(marking_img, (offset_x, offset_y - text_height - 10), (offset_x + text_width, offset_y), (140, 133, 133), -1)\n",
    "        cv2.putText(box, text, (offset_x, offset_y - 10), cv2.FONT_HERSHEY_SIMPLEX, scale, (0,255,0), thickness)\n",
    "        identified_labels[letters[i]] = classconv[event]\n",
    "        cv2.imshow(\"Marked Image\", marking_img)\n",
    "        \n",
    "        cv2.waitKey(500)  # delay for 500 milliseconds\n",
    "        \n",
    "        \n",
    "        i+= 1\n",
    "\n",
    "    cv2.imshow(\"Marked Image\", marking_img)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "##################################################\n",
    "    return identified_labels\n",
    "\n",
    "\n",
    "###############\tMain Function\t#################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1574,  0.0380, -0.0866, -0.8674,  0.7351]], device='cuda:0')\n",
      "tensor([[-0.7591,  0.2326, -0.1845,  1.1934, -0.2516]], device='cuda:0')\n",
      "tensor([[-0.2059,  0.0937,  1.0508, -0.2665, -1.0679]], device='cuda:0')\n",
      "tensor([[-0.1198,  0.3206, -0.5308,  0.4570, -0.5564]], device='cuda:0')\n",
      "tensor([[ 1.6493, -0.3846, -0.7860, -0.2372, -0.7491]], device='cuda:0')\n",
      "{'A': 'Military Vehicles', 'B': 'Humanitarian Aid and rehabilitation', 'C': 'Fire', 'D': 'Humanitarian Aid and rehabilitation', 'E': 'Combat'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        identified_labels = task_4a_return()\n",
    "        print(identified_labels)\n",
    "    except Exception as e:\n",
    "        # print(\"Exception : \", e)\n",
    "        traceback.print_exc()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor([[ 1.6326, -1.0056, -0.0355, -0.6743, -0.6500]], device='cuda:0')\n",
    "# tensor([[ 0.0537, -2.2165,  0.6708,  2.5428, -1.7980]], device='cuda:0')\n",
    "# tensor([[-0.2596, -0.3289,  0.3010,  0.1094,  0.0779]], device='cuda:0')\n",
    "# tensor([[-0.1804, -0.9747, -0.3659,  0.8733, -0.0546]], device='cuda:0')\n",
    "# tensor([[ 2.0842, -1.2373, -0.4806, -0.3214, -1.2336]], device='cuda:0')\n",
    "\n",
    "# tensor([[ 0.8802, -1.0033,  0.1944,  0.0655, -0.4775]], device='cuda:0')\n",
    "# tensor([[ 1.6744, -1.5106, -0.9378,  1.8045, -1.5986]], device='cuda:0')\n",
    "# tensor([[-0.4217, -1.1481,  1.4155,  0.8081, -1.0755]], device='cuda:0')\n",
    "# tensor([[ 1.1298, -0.9292, -0.1685,  0.5207, -0.8932]], device='cuda:0')\n",
    "# tensor([[ 0.6960, -0.3685, -0.9700,  1.2409, -1.4795]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Combat',\n",
       " 'DestroyedBuildings',\n",
       " 'Fire',\n",
       " 'Humanitarian Aid and rehabilitation',\n",
       " 'Military vehicles and weapons']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['Combat',\n",
    " 'DestroyedBuildings',\n",
    " 'Fire',\n",
    " 'Humanitarian Aid and rehabilitation',\n",
    " 'Military vehicles and weapons']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GG_2527",
   "language": "python",
   "name": "gg_2527"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
