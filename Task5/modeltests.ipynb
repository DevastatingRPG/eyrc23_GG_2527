{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os\n",
    "import csv\n",
    "import numpy as np \n",
    "# del Zeros\n",
    "# Zeros = np.zeros((10, 10))\n",
    "from cv2 import aruco\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_v2_s  \n",
    "import math\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import super_resolution\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image as tf_image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "tf.config.run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ArUco_details(image): \n",
    "    ArUco_details_dict = {}\n",
    "    ArUco_corners = {}\n",
    "    \n",
    "    ##############\tADD YOUR CODE HERE\t##############\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "    arucoParams = aruco.DetectorParameters()\n",
    "    # GrayScale Conversion\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ArUco markers\n",
    "    corners, ids, _ = aruco.detectMarkers(gray_image, aruco_dict, parameters=arucoParams)\n",
    "\n",
    "    if ids is not None:\n",
    "        for i in range(len(ids)):\n",
    "            marker_id = int(ids[i][0])\n",
    "            marker_center = [int(coord) for coord in list(np.mean(corners[i][0], axis=0).astype(int))]\n",
    "\n",
    "            # Store details in dictionaries\n",
    "            ArUco_details_dict[marker_id] = [marker_center, 0]\n",
    "            ArUco_corners[marker_id] = [[int(corner[0]), int(corner[1])] for corner in corners[i][0]]\n",
    "    ##################################################\n",
    "    \n",
    "    return ArUco_details_dict, ArUco_corners "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_4a_return(image_path, threshold):\n",
    "    # global cap\n",
    "    identified_labels = {}  \n",
    "    \n",
    "    # ret, frame = cap.read()\n",
    "    # display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # # Create a named window\n",
    "    # cv2.namedWindow(\"Live Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # # Flag to check if the picture has been taken\n",
    "    # picture_taken = False\n",
    "    # # Get start time\n",
    "    # start_time = time.time()\n",
    "    # # Read and display frames from the camera\n",
    "\n",
    "    # while not picture_taken:\n",
    "    #     ret, frame = cap.read()\n",
    "    #     display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    #     if not ret:\n",
    "    #         print(\"Error reading frame from the camera\")\n",
    "    #         break\n",
    "\n",
    "    #     cv2.imshow(\"Live Feed\", display_frame)\n",
    "\n",
    "    #     # Move the window to the left\n",
    "    #     cv2.moveWindow(\"Live Feed\", 0, 0)\n",
    "\n",
    "    #     if time.time() - start_time >= 5:\n",
    "    #         cv2.imwrite(image_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "    #         picture_taken = True\n",
    "\n",
    "    #     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #         break\n",
    "\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    # os.remove(image_path)\n",
    "    # cv2.imshow(\"Marked\", img)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    _, corners = detect_ArUco_details(img)\n",
    "    # mark = mark_ArUco_image(img,  details, corners)\n",
    "\n",
    "    marking_img = np.copy(img)\n",
    "    _, corners = detect_ArUco_details(marking_img)\n",
    "    \n",
    "    events = [\n",
    "        [[corners[25][3][0], corners[21][0][1]], [corners[21][0][0], corners[7][1][1]-10]],\n",
    "        [[corners[31][1][0], corners[28][1][1]], [corners[30][0][0], corners[14][3][1]]],\n",
    "        [corners[31][1], [corners[30][0][0], corners[11][3][1]]], \n",
    "        [[corners[25][0][0], corners[34][0][1]], [corners[34][0][0], corners[11][3][1]]], \n",
    "        [[corners[42][1][0], corners[52][1][1]], [corners[40][0][0], corners[10][3][1]-30]]   \n",
    "    ]\n",
    "\n",
    "    i=1\n",
    "    eventlist=[]\n",
    "    letters = {1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\", 5: \"E\"}\n",
    "    classconv = { \"combat\": \"Combat\", \"destroyedbuilding\": \"Destroyed buildings\", \n",
    "                 \"humanitarianaid\": \"Humanitarian Aid and rehabilitation\",\n",
    "                 \"militaryvehicles\": \"Military Vehicles\", \"fire\": \"Fire\", \"blank\": \"Blank\"}\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = efficientnet_v2_s().to(device)\n",
    "    \n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=5, bias=True),\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # model.classifier = torch.nn.Sequential(\n",
    "    #     nn.Linear(1280, 256), # Additional linear layer with 256 output features\n",
    "    #     # nn.ReLU(inplace=True),         # Activation function\n",
    "    #     nn.Dropout(p=0.5, inplace=True),\n",
    "    #     nn.Linear(256, 5)\n",
    "    # ).to(device)\n",
    "\n",
    "    # model.load_state_dict(torch.load('weights/gajjar.tf'))\n",
    "    # model.load_state_dict(torch.load('weights/w1.tf'))\n",
    "    model = torch.load('weights/model.pth')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224), antialias=False),\n",
    "            \n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    temp = 'output/temp.jpg'\n",
    "\n",
    "    for i, (tl, br) in enumerate(events):\n",
    "        tl_adj = [tl[0] + 10, tl[1] + 7]\n",
    "        br_adj = [br[0] - 10, br[1] - 4]\n",
    "        roi = img[tl_adj[1]:br_adj[1], tl_adj[0]:br_adj[0]]\n",
    "\n",
    "        reference = cv2.imread(f\"images/empty/{i}.jpg\")\n",
    "        \n",
    "        # Perform morphological opening\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        opened = cv2.morphologyEx(roi, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        gray = cv2.cvtColor(opened, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply adaptive thresholding to the image\n",
    "        binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "        # Find contours in the image\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Sort contours by area in descending order, take the first one (the largest)\n",
    "        contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "\n",
    "        # Get the bounding rectangle of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Crop the image using the bounding rectangle, add some padding if needed\n",
    "        padding = 0  # adjust this value according to your needs\n",
    "        crop = roi[max(0, y-padding):min(y+h+padding, roi.shape[0]), max(0, x-padding):min(x+w+padding, roi.shape[1])]\n",
    "\n",
    "        # Convert the cropped image to HSV color space\n",
    "        # hsv_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # img_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        # lower_green = np.array([0, 50, 0])\n",
    "        # upper_green = np.array([50, 100, 50])\n",
    "\n",
    "        img_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        lower_green = np.array([200, 200, 200])\n",
    "        upper_green = np.array([255, 255, 255])\n",
    "\n",
    "        # Threshold the RGB image to get only green colors\n",
    "        mask = cv2.inRange(img_rgb, lower_green, upper_green)\n",
    "\n",
    "        # Calculate the percentage of green pixels\n",
    "        green_percentage = (np.count_nonzero(mask) / (crop.shape[0] * crop.shape[1])) * 100\n",
    "        # print(green_percentage)\n",
    "\n",
    "        # If the majority of the cropped area is green, skip the classification\n",
    "        threshold_percentage = 80  # Adjust the threshold as needed\n",
    "        cv2.imwrite(temp, crop, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "        crop = cv2.imread(temp)\n",
    "\n",
    "        eventlist.append(crop)\n",
    "        cv2.imwrite(temp, crop, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "        result = cv2.imread(temp, cv2.IMREAD_COLOR)\n",
    "        result = super_resolution.cartoon_upsampling_4x(temp, temp )\n",
    "\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "            transformed_image = image_transform(result).unsqueeze(dim=0)\n",
    "            # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
    "            target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "        # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "        target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "        \n",
    "        # 9. Convert prediction probabilities -> prediction labels\n",
    "        pred = torch.argmax(target_image_pred_probs, dim=1)\n",
    "        class_names = ['combat', 'destroyedbuilding', 'fire', 'humanitarianaid', 'militaryvehicles']\n",
    "\n",
    "        print(target_image_pred_probs[0])\n",
    "\n",
    "        if max(target_image_pred_probs[0]) < threshold[i]:\n",
    "            event = \"blank\"\n",
    "        else:\n",
    "            \n",
    "            event = class_names[pred]\n",
    "\n",
    "        offset_x = tl_adj[0] + x - 10\n",
    "        offset_y = tl_adj[1] + y - 10    \n",
    "\n",
    "        box = cv2.rectangle(marking_img, (offset_x, offset_y), (offset_x + w + 20, offset_y + h + 20), (0, 255, 0), 2)\n",
    "        \n",
    "        offset_y -= 10\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        scale = 0.7\n",
    "        thickness = 1\n",
    "        text = classconv[event]\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, font, scale, thickness)\n",
    "\n",
    "        cv2.rectangle(marking_img, (offset_x, offset_y - text_height - 10), (offset_x + text_width, offset_y), (140, 133, 133), -1)\n",
    "        cv2.putText(box, text, (offset_x, offset_y - 10), cv2.FONT_HERSHEY_SIMPLEX, scale, (0,255,0), thickness)\n",
    "        identified_labels[letters[i+1]] = classconv[event]\n",
    "        # display_frame = cv2.resize(marking_img, (960, 540))\n",
    "        cv2.imshow(\"Marked Image\", marking_img)\n",
    "        \n",
    "        cv2.waitKey(500)  # delay for 500 milliseconds\n",
    "        \n",
    "    # display_frame = \n",
    "    cv2.imshow(\"Marked Image\", marking_img)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return identified_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_4a_returnkeras(image_path, threshold):\n",
    "    # ... (the rest of the function stays the same until the model loading part)\n",
    "        # global cap\n",
    "    identified_labels = {}  \n",
    "    \n",
    "    # ret, frame = cap.read()\n",
    "    # display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # # Create a named window\n",
    "    # cv2.namedWindow(\"Live Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # # Flag to check if the picture has been taken\n",
    "    # picture_taken = False\n",
    "    # # Get start time\n",
    "    # start_time = time.time()\n",
    "    # # Read and display frames from the camera\n",
    "\n",
    "    # while not picture_taken:\n",
    "    #     ret, frame = cap.read()\n",
    "    #     display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    #     if not ret:\n",
    "    #         print(\"Error reading frame from the camera\")\n",
    "    #         break\n",
    "\n",
    "    #     cv2.imshow(\"Live Feed\", display_frame)\n",
    "\n",
    "    #     # Move the window to the left\n",
    "    #     cv2.moveWindow(\"Live Feed\", 0, 0)\n",
    "\n",
    "    #     if time.time() - start_time >= 5:\n",
    "    #         cv2.imwrite(image_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "    #         picture_taken = True\n",
    "\n",
    "    #     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #         break\n",
    "\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    # os.remove(image_path)\n",
    "    # cv2.imshow(\"Marked\", img)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    _, corners = detect_ArUco_details(img)\n",
    "    # mark = mark_ArUco_image(img,  details, corners)\n",
    "\n",
    "    marking_img = np.copy(img)\n",
    "    _, corners = detect_ArUco_details(marking_img)\n",
    "    \n",
    "    events = [\n",
    "        [[corners[25][3][0], corners[21][0][1]], [corners[21][0][0], corners[7][1][1]-10]],\n",
    "        [[corners[31][1][0], corners[28][1][1]], [corners[30][0][0], corners[14][3][1]]],\n",
    "        [corners[31][1], [corners[30][0][0], corners[11][3][1]]], \n",
    "        [[corners[25][0][0], corners[34][0][1]], [corners[34][0][0], corners[11][3][1]]], \n",
    "        [[corners[42][1][0], corners[52][1][1]], [corners[40][0][0], corners[10][3][1]-30]]   \n",
    "    ]\n",
    "\n",
    "    i=1\n",
    "    eventlist=[]\n",
    "    letters = {1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\", 5: \"E\"}\n",
    "    classconv = { \"combat\": \"Combat\", \"destroyedbuilding\": \"Destroyed buildings\", \n",
    "                 \"humanitarianaid\": \"Humanitarian Aid and rehabilitation\",\n",
    "                 \"militaryvehicles\": \"Military Vehicles\", \"fire\": \"Fire\", \"blank\": \"Blank\"}\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the TensorFlow model\n",
    "    model = tf.keras.models.load_model('models/bruh99.h5', compile=False)\n",
    "    # IMG_SIZE = (224, 224)\n",
    "\n",
    "\n",
    "    # base_model = tf.keras.applications.EfficientNetV2S(input_shape=IMG_SHAPE,\n",
    "    #                                            include_top=False,\n",
    "    #                                            weights='imagenet')\n",
    "\n",
    "    # ... (the rest of the function stays the same until the image preprocessing part)\n",
    "    \n",
    "    # ret, frame = cap.read()\n",
    "    # display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # # Create a named window\n",
    "    # cv2.namedWindow(\"Live Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # # Flag to check if the picture has been taken\n",
    "    # picture_taken = False\n",
    "    # # Get start time\n",
    "    # start_time = time.time()\n",
    "    # # Read and display frames from the camera\n",
    "\n",
    "    # while not picture_taken:\n",
    "    #     ret, frame = cap.read()\n",
    "    #     display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    #     if not ret:\n",
    "    #         print(\"Error reading frame from the camera\")\n",
    "    #         break\n",
    "\n",
    "    #     cv2.imshow(\"Live Feed\", display_frame)\n",
    "\n",
    "    #     # Move the window to the left\n",
    "    #     cv2.moveWindow(\"Live Feed\", 0, 0)\n",
    "\n",
    "    #     if time.time() - start_time >= 5:\n",
    "    #         cv2.imwrite(image_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "    #         picture_taken = True\n",
    "\n",
    "    #     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #         break\n",
    "\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    # os.remove(image_path)\n",
    "    # cv2.imshow(\"Marked\", img)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    _, corners = detect_ArUco_details(img)\n",
    "    # mark = mark_ArUco_image(img,  details, corners)\n",
    "\n",
    "    marking_img = np.copy(img)\n",
    "    _, corners = detect_ArUco_details(marking_img)\n",
    "    \n",
    "    events = [\n",
    "        [[corners[25][3][0], corners[21][0][1]], [corners[21][0][0], corners[7][1][1]-10]],\n",
    "        [[corners[31][1][0], corners[28][1][1]], [corners[30][0][0], corners[14][3][1]]],\n",
    "        [corners[31][1], [corners[30][0][0], corners[11][3][1]]], \n",
    "        [[corners[25][0][0], corners[34][0][1]], [corners[34][0][0], corners[11][3][1]]], \n",
    "        [[corners[42][1][0], corners[52][1][1]], [corners[40][0][0], corners[10][3][1]-30]]   \n",
    "    ]\n",
    "\n",
    "    i=1\n",
    "    eventlist=[]\n",
    "    letters = {1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\", 5: \"E\"}\n",
    "    classconv = { \"combat\": \"Combat\", \"destroyedbuilding\": \"Destroyed buildings\", \n",
    "                 \"humanitarianaid\": \"Humanitarian Aid and rehabilitation\",\n",
    "                 \"militaryvehicles\": \"Military Vehicles\", \"fire\": \"Fire\", \"blank\": \"Blank\"}\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # model.classifier = torch.nn.Sequential(\n",
    "    #     nn.Linear(1280, 256), # Additional linear layer with 256 output features\n",
    "    #     # nn.ReLU(inplace=True),         # Activation function\n",
    "    #     nn.Dropout(p=0.5, inplace=True),\n",
    "    #     nn.Linear(256, 5)\n",
    "    # ).to(device)\n",
    "    # model.load_state_dict(torch.load('weights/w1.tf'))\n",
    "\n",
    "\n",
    "    \n",
    "    image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224), antialias=False),\n",
    "            \n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    temp = 'output/temp.jpg'\n",
    "\n",
    "    for i, (tl, br) in enumerate(events):\n",
    "        tl_adj = [tl[0] + 10, tl[1] + 7]\n",
    "        br_adj = [br[0] - 10, br[1] - 4]\n",
    "        roi = img[tl_adj[1]:br_adj[1], tl_adj[0]:br_adj[0]]\n",
    "\n",
    "        reference = cv2.imread(f\"images/empty/{i}.jpg\")\n",
    "        \n",
    "        # Perform morphological opening\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        opened = cv2.morphologyEx(roi, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        gray = cv2.cvtColor(opened, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply adaptive thresholding to the image\n",
    "        binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "        # Find contours in the image\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Sort contours by area in descending order, take the first one (the largest)\n",
    "        contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "\n",
    "        # Get the bounding rectangle of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Crop the image using the bounding rectangle, add some padding if needed\n",
    "        padding = 0  # adjust this value according to your needs\n",
    "        crop = roi[max(0, y-padding):min(y+h+padding, roi.shape[0]), max(0, x-padding):min(x+w+padding, roi.shape[1])]\n",
    "\n",
    "        # Convert the cropped image to HSV color space\n",
    "        # hsv_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # img_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        # lower_green = np.array([0, 50, 0])\n",
    "        # upper_green = np.array([50, 100, 50])\n",
    "\n",
    "        img_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        lower_green = np.array([200, 200, 200])\n",
    "        upper_green = np.array([255, 255, 255])\n",
    "\n",
    "        # Threshold the RGB image to get only green colors\n",
    "        mask = cv2.inRange(img_rgb, lower_green, upper_green)\n",
    "\n",
    "        # Calculate the percentage of green pixels\n",
    "        green_percentage = (np.count_nonzero(mask) / (crop.shape[0] * crop.shape[1])) * 100\n",
    "        # print(green_percentage)\n",
    "\n",
    "        # If the majority of the cropped area is green, skip the classification\n",
    "        threshold_percentage = 80  # Adjust the threshold as needed\n",
    "        cv2.imwrite(temp, crop, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "        crop = cv2.imread(temp)\n",
    "        # Preprocess the image for TensorFlow\n",
    "        tf_img = tf_image.load_img(temp, target_size=(224, 224))\n",
    "        img_array = tf_image.img_to_array(tf_img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        # Make a prediction\n",
    "        preds = model.predict(img_array)\n",
    "        # preds = model.predict(crop)\n",
    "\n",
    "        # Convert prediction probabilities to prediction labels\n",
    "        pred = np.argmax(preds, axis=1)\n",
    "        class_names = ['combat', 'destroyedbuilding', 'fire', 'humanitarianaid', 'militaryvehicles']\n",
    "\n",
    "\n",
    "        print(pred[0])\n",
    "        event = class_names[pred[0]]\n",
    "\n",
    "        offset_x = tl_adj[0] + x - 10\n",
    "        offset_y = tl_adj[1] + y - 10    \n",
    "\n",
    "        box = cv2.rectangle(marking_img, (offset_x, offset_y), (offset_x + w + 20, offset_y + h + 20), (0, 255, 0), 2)\n",
    "        \n",
    "        offset_y -= 10\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        scale = 0.7\n",
    "        thickness = 1\n",
    "        text = classconv[event]\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, font, scale, thickness)\n",
    "\n",
    "        cv2.rectangle(marking_img, (offset_x, offset_y - text_height - 10), (offset_x + text_width, offset_y), (140, 133, 133), -1)\n",
    "        cv2.putText(box, text, (offset_x, offset_y - 10), cv2.FONT_HERSHEY_SIMPLEX, scale, (0,255,0), thickness)\n",
    "        identified_labels[letters[i+1]] = classconv[event]\n",
    "        # display_frame = cv2.resize(marking_img, (960, 540))\n",
    "        cv2.imshow(\"Marked Image\", marking_img)\n",
    "        \n",
    "        cv2.waitKey(500)  # delay for 500 milliseconds\n",
    "        \n",
    "    # display_frame = \n",
    "    cv2.imshow(\"Marked Image\", marking_img)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return identified_labels\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # ... (the rest of the function stays the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty(image_path, threshold):\n",
    "    # global cap\n",
    "    identified_labels = {}  \n",
    "    \n",
    "    # ret, frame = cap.read()\n",
    "    # display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # # Create a named window\n",
    "    # cv2.namedWindow(\"Live Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # # Flag to check if the picture has been taken\n",
    "    # picture_taken = False\n",
    "    # # Get start time\n",
    "    # start_time = time.time()\n",
    "    # # Read and display frames from the camera\n",
    "\n",
    "    # while not picture_taken:\n",
    "    #     ret, frame = cap.read()\n",
    "    #     display_frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    #     if not ret:\n",
    "    #         print(\"Error reading frame from the camera\")\n",
    "    #         break\n",
    "\n",
    "    #     cv2.imshow(\"Live Feed\", display_frame)\n",
    "\n",
    "    #     # Move the window to the left\n",
    "    #     cv2.moveWindow(\"Live Feed\", 0, 0)\n",
    "\n",
    "    #     if time.time() - start_time >= 5:\n",
    "    #         cv2.imwrite(image_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "    #         picture_taken = True\n",
    "\n",
    "    #     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #         break\n",
    "\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "    img = cv2.imread(image_path)\n",
    "    # os.remove(image_path)\n",
    "    cv2.imshow(\"Marked\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    _, corners = detect_ArUco_details(img)\n",
    "    # mark = mark_ArUco_image(img,  details, corners)\n",
    "\n",
    "    marking_img = np.copy(img)\n",
    "    _, corners = detect_ArUco_details(marking_img)\n",
    "    \n",
    "    events = [\n",
    "        [[corners[25][3][0], corners[21][0][1]], [corners[21][0][0], corners[7][1][1]-10]],\n",
    "        [[corners[31][1][0], corners[28][1][1]], [corners[30][0][0], corners[14][3][1]]],\n",
    "        [corners[31][1], [corners[30][0][0], corners[11][3][1]]], \n",
    "        [[corners[25][0][0], corners[34][0][1]], [corners[34][0][0], corners[11][3][1]]], \n",
    "        [[corners[42][1][0], corners[52][1][1]], [corners[40][0][0], corners[10][3][1]-30]]   \n",
    "    ]\n",
    "\n",
    "    i=1\n",
    "    eventlist=[]\n",
    "    letters = {1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\", 5: \"E\"}\n",
    "    classconv = { \"combat\": \"Combat\", \"destroyedbuilding\": \"Destroyed buildings\", \n",
    "                 \"humanitarianaid\": \"Humanitarian Aid and rehabilitation\",\n",
    "                 \"militaryvehicles\": \"Military Vehicles\", \"fire\": \"Fire\", \"blank\": \"Blank\"}\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = efficientnet_v2_s().to(device)\n",
    "    \n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=5, bias=True),\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # model.classifier = torch.nn.Sequential(\n",
    "    #     nn.Linear(1280, 256), # Additional linear layer with 256 output features\n",
    "    #     # nn.ReLU(inplace=True),         # Activation function\n",
    "    #     nn.Dropout(p=0.5, inplace=True),\n",
    "    #     nn.Linear(256, 5)\n",
    "    # ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load('weights/weights.tf'))\n",
    "    # model.load_state_dict(torch.load('weights/w1.tf'))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224), antialias=False),\n",
    "            \n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    temp = 'output/temp.jpg'\n",
    "\n",
    "    empties = list(input(\"Empties : \"))\n",
    "\n",
    "    for i, (tl, br) in enumerate(events):\n",
    "        tl_adj = [tl[0] + 10, tl[1] + 7]\n",
    "        br_adj = [br[0] - 10, br[1] - 4]\n",
    "        roi = img[tl_adj[1]:br_adj[1], tl_adj[0]:br_adj[0]]\n",
    "\n",
    "        reference = cv2.imread(f\"images/empty/{i}.jpg\")\n",
    "        \n",
    "        # Perform morphological opening\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        opened = cv2.morphologyEx(roi, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        gray = cv2.cvtColor(opened, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply adaptive thresholding to the image\n",
    "        binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "        # Find contours in the image\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Sort contours by area in descending order, take the first one (the largest)\n",
    "        contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "\n",
    "        # Get the bounding rectangle of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Crop the image using the bounding rectangle, add some padding if needed\n",
    "        padding = 0  # adjust this value according to your needs\n",
    "        crop = roi[max(0, y-padding):min(y+h+padding, roi.shape[0]), max(0, x-padding):min(x+w+padding, roi.shape[1])]\n",
    "\n",
    "        if letters[i+1] in empties:\n",
    "            filename = f\"images/emptyds/image_{int(time.time())}.jpg\"\n",
    "            cv2.imwrite(filename, crop)\n",
    "\n",
    "        # Convert the cropped image to HSV color space\n",
    "        # hsv_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # img_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        # lower_green = np.array([0, 50, 0])\n",
    "        # upper_green = np.array([50, 100, 50])\n",
    "\n",
    "        img_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        lower_green = np.array([200, 200, 200])\n",
    "        upper_green = np.array([255, 255, 255])\n",
    "\n",
    "        # Threshold the RGB image to get only green colors\n",
    "        mask = cv2.inRange(img_rgb, lower_green, upper_green)\n",
    "\n",
    "        # Calculate the percentage of green pixels\n",
    "        green_percentage = (np.count_nonzero(mask) / (crop.shape[0] * crop.shape[1])) * 100\n",
    "        # print(green_percentage)\n",
    "\n",
    "        # If the majority of the cropped area is green, skip the classification\n",
    "        threshold_percentage = 80  # Adjust the threshold as needed\n",
    "        cv2.imwrite(temp, crop, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "        crop = cv2.imread(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        eventlist.append(crop)\n",
    "        cv2.imwrite(temp, crop, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "        result = cv2.imread(temp, cv2.IMREAD_COLOR)\n",
    "        result = super_resolution.cartoon_upsampling_4x(temp, temp )\n",
    "\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "            transformed_image = image_transform(result).unsqueeze(dim=0)\n",
    "            # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
    "            target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "        # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "        target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "        \n",
    "        # 9. Convert prediction probabilities -> prediction labels\n",
    "        pred = torch.argmax(target_image_pred_probs, dim=1)\n",
    "        class_names = ['combat', 'destroyedbuilding', 'fire', 'humanitarianaid', 'militaryvehicles']\n",
    "\n",
    "        print(target_image_pred_probs[0])\n",
    "\n",
    "        if max(target_image_pred_probs[0]) < threshold[i]:\n",
    "            event = \"blank\"\n",
    "        else:\n",
    "            \n",
    "            event = class_names[pred]\n",
    "\n",
    "        offset_x = tl_adj[0] + x - 10\n",
    "        offset_y = tl_adj[1] + y - 10    \n",
    "\n",
    "        box = cv2.rectangle(marking_img, (offset_x, offset_y), (offset_x + w + 20, offset_y + h + 20), (0, 255, 0), 2)\n",
    "        \n",
    "        offset_y -= 10\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        scale = 0.7\n",
    "        thickness = 1\n",
    "        text = classconv[event]\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, font, scale, thickness)\n",
    "\n",
    "        cv2.rectangle(marking_img, (offset_x, offset_y - text_height - 10), (offset_x + text_width, offset_y), (140, 133, 133), -1)\n",
    "        cv2.putText(box, text, (offset_x, offset_y - 10), cv2.FONT_HERSHEY_SIMPLEX, scale, (0,255,0), thickness)\n",
    "        identified_labels[letters[i+1]] = classconv[event]\n",
    "        # display_frame = cv2.resize(marking_img, (960, 540))\n",
    "        cv2.imshow(\"Marked Image\", marking_img)\n",
    "        \n",
    "        cv2.waitKey(500)  # delay for 500 milliseconds\n",
    "        \n",
    "    # display_frame = \n",
    "    cv2.imshow(\"Marked Image\", marking_img)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return identified_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_ArUco_image(image,ArUco_details_dict, ArUco_corners):\n",
    "\n",
    "    for ids, details in ArUco_details_dict.items():\n",
    "        center = details[0]\n",
    "        cv2.circle(image, center, 5, (0,0,255), -1)\n",
    "\n",
    "        corner = ArUco_corners[int(ids)]\n",
    "        cv2.circle(image, (int(corner[0][0]), int(corner[0][1])), 5, (50, 50, 50), -1)\n",
    "        cv2.circle(image, (int(corner[1][0]), int(corner[1][1])), 5, (0, 255, 0), -1)\n",
    "        cv2.circle(image, (int(corner[2][0]), int(corner[2][1])), 5, (128, 0, 255), -1)\n",
    "        cv2.circle(image, (int(corner[3][0]), int(corner[3][1])), 5, (25, 255, 255), -1)\n",
    "\n",
    "        tl_tr_center_x = int((corner[0][0] + corner[1][0]) / 2)\n",
    "        tl_tr_center_y = int((corner[0][1] + corner[1][1]) / 2) \n",
    "\n",
    "        cv2.line(image,center,(tl_tr_center_x, tl_tr_center_y),(255,0,0),5)\n",
    "        display_offset = int(math.sqrt((tl_tr_center_x - center[0])**2+(tl_tr_center_y - center[1])**2))\n",
    "        cv2.putText(image,str(ids),(center[0]+int(display_offset/2),center[1]),cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2)\n",
    "        angle = details[1]\n",
    "        cv2.putText(image,str(angle),(center[0]-display_offset,center[1]),cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread(\"images/all4.jpg\")\n",
    "# details, corners = detect_ArUco_details(img)\n",
    "# marked = mark_ArUco_image(img, details, corners)\n",
    "# cv2.imshow(\"Marked\", marked)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown object: 'Functional'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/new/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtask_4a_returnkeras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# task_4a_return(new_file, [0]*5)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[58], line 71\u001b[0m, in \u001b[0;36mtask_4a_returnkeras\u001b[1;34m(image_path, threshold)\u001b[0m\n\u001b[0;32m     68\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Load the TensorFlow model\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/bruh99.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# IMG_SIZE = (224, 224)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# cv2.destroyAllWindows()\u001b[39;00m\n\u001b[0;32m    117\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         )\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 246\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\serialization_lib.py:532\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# Fall back to legacy deserialization for all TF1 users or if\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# wrapped by in_tf_saved_model_scope() to explicitly use legacy\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# saved_model logic.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mtf2\u001b[38;5;241m.\u001b[39menabled() \u001b[38;5;129;01mor\u001b[39;00m in_tf_saved_model_scope():\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_serialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintable_module_name\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\legacy\\serialization.py:480\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     config \u001b[38;5;241m=\u001b[39m identifier\n\u001b[1;32m--> 480\u001b[0m     (\u001b[38;5;28mcls\u001b[39m, cls_config) \u001b[38;5;241m=\u001b[39m \u001b[43mclass_and_config_for_serialized_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintable_module_name\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;66;03m# If this object has already been loaded (i.e. it's shared between\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;66;03m# multiple objects), return the already-loaded object.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m     shared_object_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(SHARED_OBJECT_KEY)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\legacy\\serialization.py:365\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_object(\n\u001b[0;32m    362\u001b[0m     class_name, custom_objects, module_objects\n\u001b[0;32m    363\u001b[0m )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m     )\n\u001b[0;32m    373\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown object: 'Functional'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"images/new\")\n",
    "for file in files:\n",
    "    new_file = str(\"images/new/\" + file)\n",
    "    try:\n",
    "        task_4a_returnkeras(new_file, [0]*5)\n",
    "        # task_4a_return(new_file, [0]*5)\n",
    "    except IndexError:\n",
    "        continue\n",
    "    except KeyError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_4a_return(\"images/captured.jpg\", [0]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GG_2527",
   "language": "python",
   "name": "gg_2527"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
